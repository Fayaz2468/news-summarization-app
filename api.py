# -*- coding: utf-8 -*-
"""api

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mbR_yB62FK5a7scLajrvKfOFSvs3PfkV
"""

from fastapi import FastAPI
import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from transformers import pipeline
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from collections import Counter
from gtts import gTTS
from googletrans import Translator
import os
import time

app = FastAPI()

# Initialize models
analyzer = SentimentIntensityAnalyzer()
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
translator = Translator()

def get_article_text(url):
    """Fetches full article text from the given URL."""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        full_text = " ".join(p.text.strip() for p in paragraphs if len(p.text) > 10)

        return full_text if full_text else None
    except:
        return None

def summarize_text(text, num_sentences=2):
    """Summarizes the article using LSA-based NLP summarization."""
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = LsaSummarizer()
        summary = summarizer(parser.document, num_sentences)
        return " ".join(str(sentence) for sentence in summary) if summary else None
    except:
        return None

def analyze_sentiment(text):
    """Performs sentiment analysis and returns category + score."""
    scores = analyzer.polarity_scores(text)
    compound = scores["compound"]

    if compound >= 0.05:
        sentiment = "Positive"
    elif compound <= -0.05:
        sentiment = "Negative"
    else:
        sentiment = "Neutral"

    return sentiment, compound

def get_topics(article_text):
    """Classifies the text into topics using a zero-shot classifier."""
    candidate_labels = ["Finance", "Technology", "Healthcare", "Politics", "Sports", "Automobile", "Stock Market", "Business", "Entertainment", "Environment", "Crime"]

    try:
        zero_shot_result = classifier(article_text, candidate_labels)
        return zero_shot_result["labels"][:3]  # Top 3 predicted topics
    except:
        return ["General News"]

def get_bing_news(company_name):
    """Fetches 10 news articles with summaries, sentiment, and topic analysis."""
    base_url = "https://www.bing.com/news/search?q="
    headers = {"User-Agent": "Mozilla/5.0"}

    valid_articles = []
    page = 0

    while len(valid_articles) < 10:
        search_url = f"{base_url}{company_name.replace(' ', '+')}&first={page * 10}"
        response = requests.get(search_url, headers=headers)

        if response.status_code != 200:
            break

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.find_all("a", {"class": "title"})

        for article in articles:
            if len(valid_articles) >= 10:
                break

            title = article.text.strip()
            link = article["href"]

            article_text = get_article_text(link)
            if not article_text:
                continue

            summary = summarize_text(article_text)
            if not summary:
                continue

            sentiment, score = analyze_sentiment(summary)
            topics = get_topics(article_text) if article_text else ["General News"]

            valid_articles.append({
                "Title": title,
                "Summary": summary,
                "Sentiment": sentiment,
                "Topics": topics
            })

        page += 1
        time.sleep(1)

    return valid_articles

@app.get("/news/")
def get_news(company: str):
    articles = get_bing_news(company)
    sentiment_counts = Counter(article["Sentiment"] for article in articles)

    coverage_differences = [
        {
            "Comparison": f"Article {i+1} highlights {articles[i]['Topics']}, while Article {i+2} discusses {articles[i+1]['Topics']}",
            "Impact": f"One article suggests a {articles[i]['Sentiment'].lower()} outlook, whereas the other presents a {articles[i+1]['Sentiment'].lower()} perspective."
        }
        for i in range(0, len(articles) - 1, 2)
    ]

    unique_topics_per_article = {
        f"Unique Topics in Article {i+1}": list(set(articles[i]["Topics"]) - set(articles[i-1]["Topics"]))
        for i in range(1, len(articles))
    }

    if sentiment_counts["Positive"] > sentiment_counts["Negative"]:
        final_summary = f"{company}’s latest news coverage is mostly positive. Potential stock growth expected."
    elif sentiment_counts["Negative"] > sentiment_counts["Positive"]:
        final_summary = f"{company}’s latest news coverage leans negative, indicating possible risks."
    else:
        final_summary = f"{company}’s latest news coverage is balanced with mixed perspectives."

    return {
        "Company": company,
        "Articles": articles,
        "Comparative Sentiment Score": {
            "Sentiment Distribution": dict(sentiment_counts),
            "Coverage Differences": coverage_differences,
            "Topic Overlap": unique_topics_per_article
        },
        "Final Sentiment Analysis": final_summary,
        "Audio": "[Play Hindi Speech]"
    }

@app.get("/tts/")
def generate_tts(company: str):
    """Generates Hindi speech from the summary using Google Translate and gTTS."""
    news_data = get_news(company)
    summary_text = f"{company} related news has {len(news_data['Articles'])} articles. "
    summary_text += " ".join([f"{a['Title']}: {a['Sentiment']}." for a in news_data["Articles"]])

    translated_summary = translator.translate(summary_text, src="en", dest="hi").text

    tts = gTTS(text=translated_summary, lang="hi")
    tts.save("summary.mp3")

    return {"audio_file": "summary.mp3"}